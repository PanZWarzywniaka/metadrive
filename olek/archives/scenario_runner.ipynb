{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metadrive.envs.metadrive_env import MetaDriveEnv\n",
    "from metadrive.component.map.base_map import BaseMap\n",
    "from metadrive.component.map.pg_map import MapGenerateMethod\n",
    "from metadrive.examples.ppo_expert.torch_expert import torch_expert as expert\n",
    "from metadrive.utils import generate_gif\n",
    "\n",
    "from metadrive.engine.logger import get_logger\n",
    "from IPython.display import Image, clear_output\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('eval_data/2137.gif'),\n",
       " PosixPath('eval_data/10.png'),\n",
       " PosixPath('eval_data/10.gif'),\n",
       " PosixPath('eval_data/69.png'),\n",
       " PosixPath('eval_data/69.json'),\n",
       " PosixPath('eval_data/10.json'),\n",
       " PosixPath('eval_data/2137.png'),\n",
       " PosixPath('eval_data/2137.json'),\n",
       " PosixPath('eval_data/69.gif')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger = get_logger()\n",
    "SAVE_DIR = Path(\"eval_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(seed: int = 0, decision_repeat: int = 5, dt: float = 0.02):\n",
    "    # ===== Fidelity Config =====\n",
    "    fidelity_params = dict(decision_repeat=decision_repeat, physics_world_step_size=dt)\n",
    "\n",
    "    # ===== Termination Scheme =====\n",
    "    termination_sceme = dict(\n",
    "        out_of_route_done=False,\n",
    "        on_continuous_line_done=False,\n",
    "        crash_vehicle_done=False,\n",
    "        crash_object_done=False,\n",
    "        crash_human_done=False,\n",
    "    )\n",
    "    # ===== Map Config =====\n",
    "    map_config = {\n",
    "        BaseMap.GENERATE_TYPE: MapGenerateMethod.BIG_BLOCK_NUM,\n",
    "        BaseMap.GENERATE_CONFIG: 5,  # 20 block\n",
    "    }\n",
    "\n",
    "    cfg = dict(\n",
    "        # use_render=True,\n",
    "        log_level=logging.INFO,  # logging.DEBUG\n",
    "        start_seed=seed,\n",
    "        map_config=map_config,\n",
    "        **termination_sceme,\n",
    "        **fidelity_params,\n",
    "    )\n",
    "    env = MetaDriveEnv(config=cfg)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_steps(env: MetaDriveEnv):\n",
    "    \"\"\"\n",
    "    Return maximum number of simulation steps.\n",
    "\n",
    "    Assume minimal target velocity e.g. 2m/s.\n",
    "    Dependant on the total route length.\n",
    "    Adaptable to fidelity parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    decision_repeat = env.config[\"decision_repeat\"]\n",
    "    dt = env.config[\"physics_world_step_size\"]\n",
    "    distance = env.agent.navigation.total_length\n",
    "    V_min = 2.0  # [m/s]  # set minimal velocity to 2m/s\n",
    "\n",
    "    max_steps = distance / (V_min * decision_repeat * dt)\n",
    "    logger.info(f\"Calculating max steps with: \")\n",
    "    logger.info(\n",
    "        f\"{V_min = }, {decision_repeat = }, {dt = }, {distance = :.2f}, {round(max_steps) = }\"\n",
    "    )\n",
    "    return round(max_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_step_info(info) -> dict:\n",
    "    \"\"\"Convert numpy floats to native so can be serialized.\"\"\"\n",
    "    info[\"action\"] = [float(x) for x in info[\"action\"]]\n",
    "    info[\"raw_action\"] = [float(x) for x in info[\"raw_action\"]]\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_action_loop(\n",
    "    env: MetaDriveEnv, max_steps: int, record_gif: bool = False\n",
    ") -> list:\n",
    "    \"\"\"Runs the simulations steps untill max_steps limit hit\"\"\"\n",
    "    steps_infos = []\n",
    "    frames = []\n",
    "    while True:\n",
    "\n",
    "        action = expert(env.agent, deterministic=True)\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        if info[\"episode_length\"] == max_steps:\n",
    "            truncated = True\n",
    "            info[\"max_step\"] = True\n",
    "\n",
    "        if record_gif:\n",
    "            frames.append(env.render(mode=\"topdown\", window=False))\n",
    "\n",
    "        steps_infos.append(serialize_step_info(info))\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    if record_gif:\n",
    "        SAVE_DIR = \"eval_data\"\n",
    "        generate_gif(frames, gif_name=f\"{SAVE_DIR}/{env.current_seed}.gif\")\n",
    "\n",
    "    return steps_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_timestamps(start_ts, initialized_ts, scenario_done_ts):\n",
    "    \"\"\"Calculate and log time it took to initialise and run the env.\"\"\"\n",
    "\n",
    "    init_time = initialized_ts - start_ts\n",
    "    logger.info(f\"Initializing the env took {init_time:.2f}s\")\n",
    "\n",
    "    scenario_time = scenario_done_ts - initialized_ts\n",
    "    logger.info(f\"Running the scenario took {scenario_time:.2f}s\")\n",
    "\n",
    "    total_time = scenario_done_ts - start_ts\n",
    "    logger.info(f\"Total scenario execution took {total_time:.2f}s\")\n",
    "\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map_img(env):\n",
    "    \"\"\"Get map image of the current environment\"\"\"\n",
    "    map = env.current_map.get_semantic_map(\n",
    "        env.current_map.get_center_point(),\n",
    "    )\n",
    "    map = map.squeeze()  # reduce dimensionality\n",
    "    map = (map * 255 * 4).astype(np.uint8)\n",
    "    img = Image.fromarray(map)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_before() -> bool:\n",
    "    \"\"\"Check if we have run the scenario before\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_scenario(\n",
    "    seed: int = 0, decision_repeat: int = 5, dt: float = 0.02, record_gif=False\n",
    "):\n",
    "\n",
    "    start_ts = time.perf_counter()\n",
    "\n",
    "    # initialize\n",
    "    env = create_env(seed, decision_repeat, dt)\n",
    "    _, reset_info = env.reset()\n",
    "\n",
    "    initialized_ts = time.perf_counter()\n",
    "\n",
    "    # running loop\n",
    "    max_step = get_max_steps(env)\n",
    "    steps_info = state_action_loop(env, max_step, record_gif)\n",
    "    scenario_done_ts = time.perf_counter()\n",
    "\n",
    "    # save metadata\n",
    "    scenario_data = process_timestamps(start_ts, initialized_ts, scenario_done_ts)\n",
    "\n",
    "    steps_info.insert(0, reset_info)\n",
    "    scenario_data[\"steps_infos\"] = steps_info\n",
    "    scenario_data[\"map_data\"] = env.current_map.get_meta_data()[\"block_sequence\"]\n",
    "    scenario_data[\"max_steps\"] = max_step\n",
    "\n",
    "    with open(f\"eval_data/{seed}.json\", \"w\") as f:\n",
    "        json.dump(scenario_data, f, indent=4)\n",
    "\n",
    "    get_map_img(env).save(f\"eval_data/{seed}.png\")\n",
    "\n",
    "    data_saved_ts = time.perf_counter()\n",
    "    logger.info(f\"Saving data took {data_saved_ts-scenario_done_ts:.2f}s\")\n",
    "    logger.info(f\"Running scenario finished.\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_scenario(seed=10)\n",
    "run_scenario(seed=69)\n",
    "run_scenario(seed=2137)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metadrive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
